# -*- coding: utf-8 -*-
"""NLP to sql using torch

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OrhMH1Dp4wyuuYMhD2u2RKK2cj4RrWXO
"""

import pandas as pd


datasets = pd.read_csv(r'/content/spider_text_sql_selected.csv')  # import datasets

datasets.head() #print top 5 rows

from transformers import Seq2SeqTrainingArguments # how the model trained(performance of the model)

training_args = Seq2SeqTrainingArguments(
    output_dir="./results",  # Directory to save the model
    evaluation_strategy="epoch",  # Evaluate after each epoch
    save_strategy="epoch",  # Save model after each epoch
    per_device_train_batch_size=8,  # Batch size for training
    per_device_eval_batch_size=8,  # Batch size for evaluation
    num_train_epochs=3,  # Number of training epochs
    predict_with_generate=True  # Enables text generation during evaluation
)

import torch
from torch.utils.data import Dataset
from transformers import AutoTokenizer


tokenizer = AutoTokenizer.from_pretrained(model_path)

def create_text2sql_dataset(df, tokenizer, max_length=512):
    dataset = []

    # Get the actual column names from the DataFrame
    input_column = df.columns[0]  # Assuming the input column is the first column
    output_column = df.columns[1]  # Assuming the output column is the second column

    for _, row in df.iterrows():
        # Use the actual column names to extract the data
        input_text = row[input_column]
        output_text = row[output_column]

        # Tokenize input and output
        encoded_input = tokenizer(input_text, max_length=max_length, padding="max_length", truncation=True, return_tensors="pt")
        encoded_output = tokenizer(output_text, max_length=max_length, padding="max_length", truncation=True, return_tensors="pt")

        dataset.append({
            "input_ids": encoded_input["input_ids"].squeeze(),
            "attention_mask": encoded_input["attention_mask"].squeeze(),
            "labels": encoded_output["input_ids"].squeeze(),
        })

    return dataset

# Example usage
dataset = create_text2sql_dataset(datasets, tokenizer)

from transformers import Seq2SeqTrainer

# Trainer setup
trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=dataset, # Training dataset
    eval_dataset=dataset,  # Evaluation dataset
    tokenizer=tokenizer
)

def generate_sql(query, schema):
    input_text = f"Question: {query} Schema: {schema}"
    input_ids = tokenizer(input_text, return_tensors="pt").input_ids
    input_ids = input_ids.to(model.device) # Move input to same device as model
    outputs = model.generate(input_ids, max_length=512)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# Example Query
user_query = "Find all customers age more than 99."  # write the query
schema = "flight"  # write the database name

predicted_sql = generate_sql(user_query, schema)
print("Predicted SQL:", predicted_sql)

